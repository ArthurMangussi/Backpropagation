# Redes Neurais e Gradiente Descendente em R e Python

Este repositório contém implementações simples de:

- **Perceptron**  
- **Multilayer Perceptron (MLP)**  
- **Backpropagation**  
- **Gradiente Descendente**  

em **R** e **Python**, para estudo e demonstração de conceitos de redes neurais.

---

## Estrutura

- `R/` : Scripts em R  
- `Python/` : Scripts em Python  
- `examples/` : Exemplos com operadores lógicos (AND, OR, XOR, etc.)
- `Material/`: Derivação matemática do Algoritmo de Backpropagation
---

## Como Usar

1. Clone o repositório:
```bash
git clone https://github.com/seu_usuario/seu_repositorio.git
```

2. Para R:
Abra os scripts no RStudio ou Jupyter Notebook com kernel R e execute.

3. Para Python:
Abra os notebooks em Jupyter e execute as células.

4. Material Teórico:
Na pasta `Material/`, você encontrará:

- Slides da derivação matemática do algoritmo;

- Lista de exercícios resolvidos e proposta na disciplina PO-249 – Introdução às Redes Neurais e Grandes Modelos de Linguagem, ministrada pelo Prof. Dr. Mauri Oliveira no Programa de Pós-Graduação em Pesquisa Operacional do ITA.


## Derivação Matemática do Algoritmo de Backpropagation

Este repositório apresenta tanto a implementação quanto a derivação matemática do algoritmo de Backpropagation, desenvolvido no contexto da disciplina PO-249: Introdução às Redes Neurais e Grandes Modelos de Linguagem, ministrada pelo Prof. Dr. Mauri Oliveira no Instituto Tecnológico de Aeronáutica (ITA).

O material foi preparado por Arthur Dantas Mangussi, doutorando em Ciências de Dados, e ministrado para a turma CMC-15 da graduação do ITA.

